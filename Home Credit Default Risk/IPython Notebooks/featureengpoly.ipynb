{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "featureengpoly.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNzjSAYv9S8V",
        "colab_type": "text"
      },
      "source": [
        "Feature engineering an important part of machine-learning as we try to modify/create (i.e., engineer) new features from our existing dataset that might be meaningful in predicting the TARGET.\n",
        "\n",
        "Each datasets provides more information about the loan application in terms of how prompt they have been on their instalment payments, their credit history on other loans, the amount of cash or credit card balances they have etc. A data scientists/researcher should always investigate and create new features from all the information provided.\n",
        "\n",
        "In this basic exercise, we will focus on the main dataset, that is application_train.csv. We will go through two simple methodologies in feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VZ-vOD29a8O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "\n",
        "\n",
        "# importing graphics modules\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mqvuu1hW9njz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_app_test_align = db['df_app_test_align'] \n",
        "df_app_train_align = db['df_app_train_align'] \n",
        "df_app_train_corr_target = db['df_app_train_corr_target']\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXtnCIVi99-q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df_app_train_corr_target.tail(10))\n",
        "print(df_app_train_corr_target.head(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9pI2PzL-BvP",
        "colab_type": "text"
      },
      "source": [
        "**Polynomial features**\n",
        "The variables that we select are EXT_SOURCE_1/2/3 (-ve), DAYS_BIRTH (+ve), and DAYS_EMPLOYED (+ve) that all have large correlation values to TARGET relative to the other features.\n",
        "\n",
        "We create new poly_feat_x dataframes as both training and test datasets need to be equivalent. Thus, any polnomial features in the training dataset, must be created for the test dataset too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SifI68B3-EvC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imp_feat_list = ['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH'] #  ,'DAYS_EMPLOYED'\n",
        "\n",
        "poly_feat_train = df_app_train_align[imp_feat_list]\n",
        "poly_feat_test = df_app_test_align[imp_feat_list]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQQCCj6l-J-q",
        "colab_type": "text"
      },
      "source": [
        "We observed that several features often had NaN values. We use the SimpleImputer function in scikit-learn's impute toolkit where we replace all np.nan with median values in that column.\n",
        "\n",
        "We fit on the training data, as that is all the in-sample data that we have. Then we perform the transformations on both the training and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRHTZ0to-OAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(missing_values=np.nan,strategy='median')\n",
        "imputer.fit(poly_feat_train)\n",
        "\n",
        "poly_feat_train = imputer.transform(poly_feat_train)\n",
        "poly_feat_test = imputer.transform(poly_feat_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01z9iOlL-RXk",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBuCA1zl-Xcf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "poly_transform = PolynomialFeatures(degree=3)\n",
        "poly_transform.fit(poly_feat_train)\n",
        "\n",
        "poly_transform_train = poly_transform.transform(poly_feat_train)\n",
        "poly_transform_test = poly_transform.transform(poly_feat_test)\n",
        "\n",
        "print('Shape of polynomial features (training): {}'.format(poly_transform_train.shape))\n",
        "print('Shape of polynomial features (test): {}'.format(poly_transform_test.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpt_bTB0-bHs",
        "colab_type": "text"
      },
      "source": [
        "Shape of polynomial features (training): (307511, 35)\n",
        "Shape of polynomial features (test): (48744, 35)\n",
        "We can see a detailed list of the new polynomial and interaction features that have been created"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-wcbaG6-dAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "poly_feat_name_list = poly_transform.get_feature_names(imp_feat_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BLkTgas-gO-",
        "colab_type": "text"
      },
      "source": [
        "We would like to see if any of these new features in the training dataset have higher correlations with the TARGET. Since the new polynomial features in the training dataset have a correlation magnitude greater than the original feature set, we should consider adding them into our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb6_bsvj-lfa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_poly_feat_train = pd.DataFrame(poly_transform_train,columns=poly_feat_name_list)\n",
        "df_poly_feat_train['TARGET'] = df_app_train_align['TARGET']\n",
        "poly_feat_corr = df_poly_feat_train.corr()['TARGET'].sort_values()\n",
        "\n",
        "print(poly_feat_corr.head(10))\n",
        "print(poly_feat_corr.tail(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaZTCz4W-sY6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_poly_feat_test = pd.DataFrame(poly_transform_test,columns=poly_feat_name_list)\n",
        "df_poly_feat_train.index = df_app_train_align.index\n",
        "df_poly_feat_test.index  = df_app_test_align.index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivyxTARb-ylC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_app_train_poly = df_app_train_align.merge(df_poly_feat_train,left_index=True,right_index=True)\n",
        "df_app_test_poly = df_app_test_align.merge(df_poly_feat_test,left_index=True,right_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_b25epdA-2hZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_app_train_poly_align, df_app_test_poly_align = df_app_train_poly.align(df_app_test_poly,join='inner',axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6ljN7iu-8ME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_app_train_poly_align.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWblet6n_AIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_app_test_poly_align.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k4moR2B_HTs",
        "colab_type": "text"
      },
      "source": [
        "We can see that our original dataset has 237 features, and our polynomial feature engineering resulted in 36 new features. Having an extended set of both original and polynomial features resulted in 273 features. When we aligned both the training and test dataets together, we eend up with 271 features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5FOIhMe_Mq7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Original features (train): {}'.format(df_app_train_align.shape))\n",
        "print('Polynomial features(train):{}'.format(df_poly_feat_train.shape))\n",
        "print('Original & polynomial features (train): {}'.format(df_app_train_poly.shape))\n",
        "print('Original & polynomial features align (train): {}'.format(df_app_train_poly_align.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myE3K7C4_UrI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s1 = set(df_app_train_poly.columns)\n",
        "s2 = set(df_app_train_poly_align.columns)\n",
        "\n",
        "diff_s1_s2 = s1-s2\n",
        "diff_s1_s2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0MozRkE_aHd",
        "colab_type": "text"
      },
      "source": [
        "**Expert knowledge features**\n",
        "Often, experts have domain knowledge about what combination of existing features have strong explanatory/predictive power. In this case we are looking at the following features\n",
        "\n",
        "Percentage of days employed - How long a person has been employed as a percentage of his life is a stronger predictor of his ability to keep paying off his loans.\n",
        "Available credit as a percentage of income - If a person has a very large amount of credit available as a percentage of income, this can impact his ability to pay off the loans\n",
        "Annuity as a percentage of income - If a person receives an annuity, this is a more stable source of income thus if it is higher, you are less likely to default.\n",
        "Annuity as a percentage of available credit - If a person receives an annuity, this is more stable source of income thus if it is a high percentage compared to his/her credit availability then the person is more likely be able to pay off his debts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CZX_Ouq_fqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_app_train_align_expert = df_app_train_align.copy()\n",
        "df_app_test_align_expert = df_app_test_align.copy()\n",
        "\n",
        "# Training dataset\n",
        "df_app_train_align_expert['DAYS_EMPLOYED_PCT'] = df_app_train_align_expert['DAYS_EMPLOYED'] / df_app_train_align_expert['DAYS_BIRTH']\n",
        "df_app_train_align_expert['CREDIT_INCOME_PCT'] = df_app_train_align_expert['AMT_CREDIT'] / df_app_train_align_expert['AMT_INCOME_TOTAL']\n",
        "df_app_train_align_expert['ANNUITY_INCOME_PCT'] = df_app_train_align_expert['AMT_ANNUITY'] / df_app_train_align_expert['AMT_INCOME_TOTAL']\n",
        "df_app_train_align_expert['CREDIT_TERM'] = df_app_train_align_expert['AMT_ANNUITY'] / df_app_train_align_expert['AMT_CREDIT']\n",
        "\n",
        "# Test dataset\n",
        "df_app_test_align_expert['DAYS_EMPLOYED_PCT'] = df_app_test_align_expert['DAYS_EMPLOYED'] / df_app_test_align_expert['DAYS_BIRTH']\n",
        "df_app_test_align_expert['CREDIT_INCOME_PCT'] = df_app_test_align_expert['AMT_CREDIT'] / df_app_test_align_expert['AMT_INCOME_TOTAL']\n",
        "df_app_test_align_expert['ANNUITY_INCOME_PCT'] = df_app_test_align_expert['AMT_ANNUITY'] / df_app_test_align_expert['AMT_INCOME_TOTAL']\n",
        "df_app_test_align_expert['CREDIT_TERM'] = df_app_test_align_expert['AMT_ANNUITY'] / df_app_test_align_expert['AMT_CREDIT']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEqt3VuZ_jIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7C9iiOOm_nu7",
        "colab_type": "text"
      },
      "source": [
        "# **Summary**\n",
        "\n",
        "**Polynomial feature engineering**\n",
        "\n",
        "Evaluate which are the features with the largest +ve and -ve correlations with TARGET.\n",
        "Extract those features and fill in any np.nan rows by imputing with the median of that column (i.e., sklearn.impute.SimpleImputer)\n",
        "Create new polynomial and interactive features (i.e., sklearn.preprocessing.PolynomialFeatures).\n",
        "Evalute whether these new polynomial and interactive features exhibit greater +ve and -ve correlations with TARGET compared to the original feature set. If so, consider creating a new dataset with these new polynomial and interactive features.\n",
        "Include row key identifiers (i.e., index) into the new polynomial feature set for both the polynomial training and test datasets (i.e., df_poly_feat_train, df_poly_feat_test)\n",
        "Merge this new polynomial feature dataset with the original feature dataset (i.e., merge df_poly_feat_train and df_app_train_align) for both training and test datasets.\n",
        "Align the new training and test datasets together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFqXgo6Y_w2N",
        "colab_type": "text"
      },
      "source": [
        "# **Expert feature engineering**\n",
        "These are features that are well known to domain knowledge to have high explanatory and predictive power.\n",
        "They are useful in combining features in the original set together, thus making your model more parsimonious\n",
        "Once you've created these expert features, compare their correlations with the TARGET and evaluate if they are greater than the individual features themselves."
      ]
    }
  ]
}