{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/home-credit-default-risk/bureau.csv\n/kaggle/input/home-credit-default-risk/HomeCredit_columns_description.csv\n/kaggle/input/home-credit-default-risk/installments_payments.csv\n/kaggle/input/home-credit-default-risk/bureau_balance.csv\n/kaggle/input/home-credit-default-risk/application_train.csv\n/kaggle/input/home-credit-default-risk/sample_submission.csv\n/kaggle/input/home-credit-default-risk/POS_CASH_balance.csv\n/kaggle/input/home-credit-default-risk/previous_application.csv\n/kaggle/input/home-credit-default-risk/credit_card_balance.csv\n/kaggle/input/home-credit-default-risk/application_test.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/home-credit-default-risk/application_train.csv')\ndf_test = pd.read_csv('../input/home-credit-default-risk/application_test.csv')","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Assign which ever datasets you want to train and test. This is because as part of feature engineering, you will often build new and different feature datasets and would like to test each one out to evaluate whether it improves model performance.\n\nAs the imputer is being fitted on the training data and used to transform both the training and test datasets, the training data needs to have the same number of features as the test dataset. This means that the TARGET column must be removed from the training dataset, and stored in train_labels for use later."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf_train['NAME_CONTRACT_TYPE'] = le.fit_transform(df_train['NAME_CONTRACT_TYPE'])\ndf_train['FLAG_OWN_CAR'] = le.fit_transform(df_train['FLAG_OWN_CAR'])\ndf_train['FLAG_OWN_REALTY'] = le.fit_transform(df_train['FLAG_OWN_REALTY'])\n\ndf_test['NAME_CONTRACT_TYPE'] = le.fit_transform(df_test['NAME_CONTRACT_TYPE'])\ndf_test['CODE_GENDER'] = le.fit_transform(df_test['CODE_GENDER'])\ndf_test['FLAG_OWN_CAR'] = le.fit_transform(df_test['FLAG_OWN_CAR'])\ndf_test['FLAG_OWN_REALTY'] = le.fit_transform(df_test['FLAG_OWN_REALTY'])","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.get_dummies(df_train)\ndf_test = pd.get_dummies(df_test)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_train.shape)\nprint(df_test.shape)","execution_count":5,"outputs":[{"output_type":"stream","text":"(307511, 243)\n(48744, 238)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_col = set(df_train.columns)\ntest_col = set(df_test.columns)\n\nexc_col = train_col-test_col\nprint(exc_col)","execution_count":6,"outputs":[{"output_type":"stream","text":"{'CODE_GENDER_M', 'TARGET', 'NAME_INCOME_TYPE_Maternity leave', 'CODE_GENDER_F', 'NAME_FAMILY_STATUS_Unknown', 'CODE_GENDER_XNA'}\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_target_labels = df_train['TARGET']\ndf_train_align, df_test_align = df_train.align(df_test,join='inner',axis=1)\ndf_train_align['TARGET'] = train_target_labels","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training dataset shape: '.format(0),df_train_align.shape)\nprint('Testing dataset shape: '.format(0),df_test_align.shape)","execution_count":8,"outputs":[{"output_type":"stream","text":"Training dataset shape:  (307511, 238)\nTesting dataset shape:  (48744, 237)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = df_train_align.copy()\ntest = df_test_align.copy()\n\nlabels = train.pop('TARGET').values # store training labels\nfeat_names = list(train.columns) #","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ids = train.index.values\ntest_ids = test.index.values","execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****FEATURE SET PREPROCESSING****"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan,strategy='median')\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range= (0,1))","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We fit the imputer and scaler on the training data, and perform the imputer and scaling transformations on both the training and test datasets.\n\nScikit-learn models only accepts arrays. So the imputer and scalers can accept DataFrames as inputs and they output the train and test variables as arrays for use into Scikit-Learn's machine learning models."},{"metadata":{"trusted":true},"cell_type":"code","source":"imputer.fit(train)\ntrain = imputer.transform(train)\ntest = imputer.transform(test)\n\nscaler.fit(train)\ntrain = scaler.transform(train)\ntest = scaler.transform(test)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"(307511, 237)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"**MODEL IMPLEMENTATION(GRADIETN BOOSTING MACHINE**\nWith the Gradient Boosting machine, we are going to perform an additional step of using K-fold cross validation (i.e., Kfold). In the other models (i.e., Logit, Random Forest) we only fitted our model on the training dataset and then evaluated the model's performance based on the test dataset.\n\nUsing Kfold, we are going to split up our test data set into multiple folds (i.e., KK). We will be fitting our model on K-1Kâˆ’1 folds and testing it on the K^{th}K \nth\n  fold (i.e., out-of-fold) and repeating the fitting process until all KK folds are explored. This method allows us to train our model more accurately without overfitting.\n\nThus, we copy our train (test_feat) dataframe to feat (test_feat) as these are more accurate descriptors since they are now feature and test feature datasets. The training dataset is split into further training and validation datasets based on the kfold.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"feat = train.copy()\ntest_feat = test.copy()","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nimport gc","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_imp_vals = np.zeros(len(feat_names)) # feature importance\ntest_pred = np.zeros(test_feat.shape[0]) # test predictions\noof = np.zeros(feat.shape[0]) # out of fold predictions\nvalid_scores = [] # validation scores\ntrain_scores = [] # training scores\nn_folds = 5","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k_fold = KFold(n_splits = n_folds, shuffle=True,random_state=100)","execution_count":17,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the KFold we are taking our training dataset, and splitting it into further training and validation datasets. The k_fold\n\nn_estimators: Number of boosted trees to fit (i.e., 1000 trees).\nreg_alpha: L1 regularization on weights (i.e., LASSO).\nreg_lambda: L2 regularization on weights (i.e, Ridge.\nsubsample: Subsample ratio of training instance. Setting it to 0.5 lgb will randomly sample half of the training data prior to growing trees. and this will prevent overfitting. Subsampling will occur once in every boosting iteration. This prevents overfitting of the tree.\nlearning_rate: Boosting learning rate.\nn_job : Number of processes to use for calculation. -1 means all processes will be used.\nrandom_state : Allows model fit to be replicated.\nclass_weight :"},{"metadata":{"trusted":true},"cell_type":"code","source":"for train_indices, valid_indices in k_fold.split(feat):\n    \n    train_feat, train_labels = feat[train_indices], labels[train_indices] # training data for the fold\n    valid_feat, valid_labels = feat[valid_indices], labels[valid_indices] # validation data for the fold\n    \n    model = lgb.LGBMClassifier(n_estimators=1000, objective='binary',\n                              class_weight='balanced',learning_rate=0.05,\n                              reg_alpha = 0.1, reg_lambda=0.1,\n                              subsample = 0.8, n_jobs = -1, random_state=50)\n\n\n    model.fit(train_feat, train_labels, eval_metric = 'auc',\n              eval_set = [(valid_feat, valid_labels), (train_feat, train_labels)],\n              eval_names = ['valid','train'], early_stopping_rounds = 100, verbose = 200)\n             # categorical_feature = cat_indices,\n    \n    # record best iteration of each fold\n    best_iter = model.best_iteration_ \n    # record the most important features of each fold\n    feat_imp_vals += model.feature_importances_/k_fold.n_splits \n    # record the out-of-fold predictions\n    oof[valid_indices] = model.predict_proba(valid_feat, num_iteration = best_iter)[:,1]/k_fold.n_splits\n    \n    # record the predictions on the test_feat dataset\n    test_pred += model.predict_proba(test_feat, num_iteration =  best_iter)[:,1]/k_fold.n_splits \n    \n    valid_score = model.best_score_['valid']['auc']\n    train_score = model.best_score_['train']['auc']\n    \n    valid_scores.append(valid_score)\n    train_scores.append(train_score)\n    \n    gc.enable()\n    del model, train_feat, valid_feat\n    gc.collect()\n    \n","execution_count":18,"outputs":[{"output_type":"stream","text":"Training until validation scores don't improve for 100 rounds\n[200]\ttrain's auc: 0.794899\ttrain's binary_logloss: 0.552147\tvalid's auc: 0.757004\tvalid's binary_logloss: 0.567464\nEarly stopping, best iteration is:\n[222]\ttrain's auc: 0.798655\ttrain's binary_logloss: 0.548504\tvalid's auc: 0.75711\tvalid's binary_logloss: 0.565247\nTraining until validation scores don't improve for 100 rounds\n[200]\ttrain's auc: 0.795077\ttrain's binary_logloss: 0.551788\tvalid's auc: 0.756384\tvalid's binary_logloss: 0.564369\nEarly stopping, best iteration is:\n[258]\ttrain's auc: 0.804371\ttrain's binary_logloss: 0.542757\tvalid's auc: 0.756918\tvalid's binary_logloss: 0.558684\nTraining until validation scores don't improve for 100 rounds\n[200]\ttrain's auc: 0.794963\ttrain's binary_logloss: 0.551713\tvalid's auc: 0.754765\tvalid's binary_logloss: 0.567765\nEarly stopping, best iteration is:\n[245]\ttrain's auc: 0.802327\ttrain's binary_logloss: 0.544465\tvalid's auc: 0.754948\tvalid's binary_logloss: 0.563343\nTraining until validation scores don't improve for 100 rounds\n[200]\ttrain's auc: 0.794675\ttrain's binary_logloss: 0.551894\tvalid's auc: 0.753675\tvalid's binary_logloss: 0.565031\nEarly stopping, best iteration is:\n[290]\ttrain's auc: 0.808966\ttrain's binary_logloss: 0.538011\tvalid's auc: 0.753987\tvalid's binary_logloss: 0.55649\nTraining until validation scores don't improve for 100 rounds\n[200]\ttrain's auc: 0.794071\ttrain's binary_logloss: 0.552925\tvalid's auc: 0.758378\tvalid's binary_logloss: 0.567625\nEarly stopping, best iteration is:\n[230]\ttrain's auc: 0.799217\ttrain's binary_logloss: 0.547972\tvalid's auc: 0.758596\tvalid's binary_logloss: 0.564614\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**Creating summary of validation and training AUC scores**"},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_auc = roc_auc_score(labels,oof) # calculate the auc based on the test dataset labels and the out-of-fold predictions\nvalid_scores.append(valid_auc) # calculate the overall validation auc score\ntrain_scores.append(np.mean(train_scores)) # calculate the overall average training auc score\n\nfold_names = list(range(n_folds))\nfold_names.append('overall')\n\nmetrics = pd.DataFrame({'fold': fold_names,\n                      'train': train_scores,\n                      'valid': valid_scores})\n","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfeat_imp = pd.DataFrame({'Feature': feat_names,'Importance':feat_imp_vals}) # creating feature importance dataframe\nprint(feat_imp.sort_values(by=['Importance'],ascending=False).head(6))\n","execution_count":23,"outputs":[{"output_type":"stream","text":"         Feature  Importance\n31  EXT_SOURCE_1       549.2\n33  EXT_SOURCE_3       499.4\n32  EXT_SOURCE_2       438.4\n10    DAYS_BIRTH       423.6\n6     AMT_CREDIT       409.0\n7    AMT_ANNUITY       367.2\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_pred}) # creating Kaggle submission dataframe","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit = submission\nprint(submit.head())\nprint(submit.shape)","execution_count":25,"outputs":[{"output_type":"stream","text":"   SK_ID_CURR    TARGET\n0           0  0.249140\n1           1  0.470817\n2           2  0.168403\n3           3  0.232149\n4           4  0.640061\n(48744, 2)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit.to_csv('lightgbm-home-loan-credit-risk.csv',index=False)","execution_count":26,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We review our Light GBM from Kaggle and find that there is a slight improvement to 0.74 compared to 0.662 (logit) or 0.688 (random-forest). We can see that substantial improvements are obtained using LightGBM with the same dataset as logit or random-forest leading us to understand why Gradient Boosted Machines are the machine learning model of choice for many data scientists."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}